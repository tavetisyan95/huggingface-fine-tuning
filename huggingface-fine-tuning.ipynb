{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 – Preparing Our Data, Model, And Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (C:\\Users\\tigra\\.cache\\huggingface\\datasets\\tweet_eval\\emotion\\1.1.0\\12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9417de1994e4604868621be2bf2aaac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing necessary tools\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Loading our dataset\n",
    "tweet_dataset = load_dataset(path=\"tweet_eval\", name=\"emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_transform', 'activation_13', 'vocab_projector', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'classifier', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Instantiating our DistilBERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 – Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 3257\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1421\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 374\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Inspecting our dataset\n",
    "print(tweet_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 3257\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Inspecting the train set\n",
    "print(tweet_dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence samples:\n",
      " [\"“Worry is a down payment on a problem you may never have'. \\xa0Joyce Meyer.  #motivation #leadership #worry\", \"My roommate: it's okay that we can't spell because we have autocorrect. #terrible #firstworldprobs\"]\n",
      "\n",
      "Label samples:\n",
      " [2, 0]\n"
     ]
    }
   ],
   "source": [
    "# Inspecting train text and labels\n",
    "print(f\"Sequence samples:\\n {tweet_dataset['train']['text'][:2]}\\n\")\n",
    "print(f\"Label samples:\\n {tweet_dataset['train']['label'][:2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a dictionary with class names for conversion\n",
    "class_names = {0: \"anger\", 1: \"joy\", 2: \"optimism\", 3: \"sadness\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for finding the length of the longest sequence in the data\n",
    "def find_max_length(dataset):\n",
    "    return len(max(dataset, key=lambda x: len(x.split())).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest sequence in train set has 33 words\n",
      "Longest sequence in val set has 32 words\n",
      "Longest sequence in test set has 36 words\n"
     ]
    }
   ],
   "source": [
    "# Obtaining the length of the longest sequences in our data splits\n",
    "train_max_length = find_max_length(tweet_dataset[\"train\"][\"text\"])\n",
    "val_max_length = find_max_length(tweet_dataset[\"validation\"][\"text\"])\n",
    "test_max_length = find_max_length(tweet_dataset[\"test\"][\"text\"])\n",
    "\n",
    "# Inspecting the length of the longest sequences\n",
    "print(f\"Longest sequence in train set has {train_max_length} words\")\n",
    "print(f\"Longest sequence in val set has {val_max_length} words\")\n",
    "print(f\"Longest sequence in test set has {test_max_length} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering, Padding, and Tokenizing Our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for discarding sequences beyond a specified length\n",
    "def filter_dataset(dataset, num_words):    \n",
    "    return dataset.filter(lambda x: len(x[\"text\"].split()) <= num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\tigra\\.cache\\huggingface\\datasets\\tweet_eval\\emotion\\1.1.0\\12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343\\cache-8b5b5191685fa0b7.arrow\n",
      "Loading cached processed dataset at C:\\Users\\tigra\\.cache\\huggingface\\datasets\\tweet_eval\\emotion\\1.1.0\\12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343\\cache-91006f42c4d168c7.arrow\n",
      "Loading cached processed dataset at C:\\Users\\tigra\\.cache\\huggingface\\datasets\\tweet_eval\\emotion\\1.1.0\\12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343\\cache-f7c7993d24aa9038.arrow\n"
     ]
    }
   ],
   "source": [
    "# Specifying the max length for sequences\n",
    "num_words = 36\n",
    "\n",
    "# Dropping sequences longer than the specified number from the dataset\n",
    "filtered_dataset = filter_dataset(tweet_dataset, num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 3257\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1421\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 374\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Inspecting the shortened dataset\n",
    "print(filtered_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A function for tokenizing our dataset\n",
    "def tokenize_dataset(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", \n",
    "                     truncation=True, max_length=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\tigra\\.cache\\huggingface\\datasets\\tweet_eval\\emotion\\1.1.0\\12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343\\cache-3fc95a2e602f9734.arrow\n",
      "Loading cached processed dataset at C:\\Users\\tigra\\.cache\\huggingface\\datasets\\tweet_eval\\emotion\\1.1.0\\12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343\\cache-b03aab1360817548.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa4ce0d33d484cd4824b5f222fb0c55d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/374 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenizing our dataset\n",
    "tokenized_dataset = filtered_dataset.map(tokenize_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 3257\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1421\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 374\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Inspecting the tokenized dataset\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"“Worry is a down payment on a problem you may never have'. \\xa0Joyce Meyer.  #motivation #leadership #worry\", 'label': 2, 'input_ids': [101, 1523, 4737, 2003, 1037, 2091, 7909, 2006, 1037, 3291, 2017, 2089, 2196, 2031, 1005, 1012, 11830, 11527, 1012, 1001, 14354, 1001, 4105, 1001, 4737, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# Inspecting a training sample\n",
    "print(tokenized_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing \"text\" and \"label\" columns from our data splits to craft features for the model\n",
    "train_features = tokenized_dataset[\"train\"].remove_columns([\"text\", \"label\"]).with_format(\"tensorflow\")\n",
    "val_features = tokenized_dataset[\"validation\"].remove_columns([\"text\", \"label\"]).with_format(\"tensorflow\")\n",
    "test_features = tokenized_dataset[\"test\"].remove_columns([\"text\", \"label\"]).with_format(\"tensorflow\")\n",
    "\n",
    "# Converting our features to TF Tensors\n",
    "train_features = {x: train_features[x].to_tensor() for x in tokenizer.model_input_names}\n",
    "val_features = {x: val_features[x].to_tensor() for x in tokenizer.model_input_names}\n",
    "test_features = {x: test_features[x].to_tensor() for x in tokenizer.model_input_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# Inspecting expected model input names\n",
    "print(tokenizer.model_input_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(3257, 36), dtype=int64, numpy=\n",
      "array([[  101,  1523,  4737, ...,     0,     0,     0],\n",
      "       [  101,  2026, 18328, ...,     0,     0,     0],\n",
      "       [  101,  2053,  2021, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101,  1030,  5310, ...,     0,     0,     0],\n",
      "       [  101,  2017,  2031, ...,     0,     0,     0],\n",
      "       [  101,  1030,  5310, ...,     0,     0,     0]], dtype=int64)>, 'attention_mask': <tf.Tensor: shape=(3257, 36), dtype=int64, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int64)>}\n"
     ]
    }
   ],
   "source": [
    "# Inspecting our Tensors\n",
    "print(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the function for one-hot encoding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Creating labels for each of the data splits\n",
    "train_labels = to_categorical(tokenized_dataset[\"train\"][\"label\"])\n",
    "val_labels = to_categorical(tokenized_dataset[\"validation\"][\"label\"])\n",
    "test_labels = to_categorical(tokenized_dataset[\"test\"][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Inspecting training labels\n",
    "print(train_labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Datasets for Training, Validation, and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the TF Dataset class\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "# Creating TF Datasets for each of our data splits\n",
    "train_dataset = Dataset.from_tensor_slices((train_features, train_labels))\n",
    "val_dataset = Dataset.from_tensor_slices((val_features, val_labels))\n",
    "test_dataset = Dataset.from_tensor_slices((test_features, test_labels))\n",
    "\n",
    "# Shuffling and batching our data\n",
    "train_dataset = train_dataset.shuffle(len(train_features), seed=2).batch(8)\n",
    "val_dataset = val_dataset.shuffle(len(train_features), seed=2).batch(8)\n",
    "test_dataset = test_dataset.shuffle(len(train_features), seed=2).batch(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 – Setting Up Model Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing DistilBERT Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "distilbert (TFDistilBertMain multiple                  66362880  \n",
      "_________________________________________________________________\n",
      "pre_classifier (Dense)       multiple                  590592    \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         multiple                  0         \n",
      "=================================================================\n",
      "Total params: 66,956,548\n",
      "Trainable params: 66,956,548\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Inspecting the model's architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing the DistilBERT block\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "distilbert (TFDistilBertMain multiple                  66362880  \n",
      "_________________________________________________________________\n",
      "pre_classifier (Dense)       multiple                  590592    \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3076      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         multiple                  0         \n",
      "=================================================================\n",
      "Total params: 66,956,548\n",
      "Trainable params: 593,668\n",
      "Non-trainable params: 66,362,880\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Inspecting the model again to see the differences in trainable params\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making A Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function defining our learning rate schedule\n",
    "def lr_decay(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * np.exp(-0.1 * epoch)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating our learning rate scheduler callback\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(schedule=lr_decay, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Performance Metrics and Compiling Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tigra\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Setting some hyperparameters and compiling the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001), \n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), \n",
    "              metrics=tf.keras.metrics.CategoricalAccuracy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 – Training, Validation, and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x0000026D20B93D60>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x0000026D20B93D60>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:From c:\\users\\tigra\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "407/408 [============================>.] - ETA: 0s - loss: 0.9867 - categorical_accuracy: 0.6087WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "408/408 [==============================] - 28s 44ms/step - loss: 0.9866 - categorical_accuracy: 0.6088 - val_loss: 0.8948 - val_categorical_accuracy: 0.6176\n",
      "Epoch 2/15\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "408/408 [==============================] - 17s 41ms/step - loss: 0.8314 - categorical_accuracy: 0.6742 - val_loss: 0.8540 - val_categorical_accuracy: 0.6444\n",
      "Epoch 3/15\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "408/408 [==============================] - 16s 39ms/step - loss: 0.8004 - categorical_accuracy: 0.6874 - val_loss: 0.8355 - val_categorical_accuracy: 0.6390\n",
      "Epoch 4/15\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "408/408 [==============================] - 16s 39ms/step - loss: 0.7789 - categorical_accuracy: 0.6933 - val_loss: 0.8153 - val_categorical_accuracy: 0.6524\n",
      "Epoch 5/15\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "408/408 [==============================] - 16s 39ms/step - loss: 0.7630 - categorical_accuracy: 0.6976 - val_loss: 0.8000 - val_categorical_accuracy: 0.6658\n",
      "Epoch 6/15\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "408/408 [==============================] - 17s 42ms/step - loss: 0.7465 - categorical_accuracy: 0.7065 - val_loss: 0.8149 - val_categorical_accuracy: 0.6471\n",
      "Epoch 7/15\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "408/408 [==============================] - 17s 41ms/step - loss: 0.7303 - categorical_accuracy: 0.7086 - val_loss: 0.8147 - val_categorical_accuracy: 0.6444\n",
      "Epoch 8/15\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "408/408 [==============================] - 16s 38ms/step - loss: 0.7182 - categorical_accuracy: 0.7163 - val_loss: 0.8223 - val_categorical_accuracy: 0.6390\n",
      "Epoch 9/15\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "408/408 [==============================] - 16s 38ms/step - loss: 0.7012 - categorical_accuracy: 0.7215 - val_loss: 0.8358 - val_categorical_accuracy: 0.6337\n",
      "Epoch 10/15\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "408/408 [==============================] - 16s 39ms/step - loss: 0.6956 - categorical_accuracy: 0.7249 - val_loss: 0.8391 - val_categorical_accuracy: 0.6497\n",
      "Epoch 11/15\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0003678794586447782.\n",
      "408/408 [==============================] - 16s 39ms/step - loss: 0.6354 - categorical_accuracy: 0.7525 - val_loss: 0.7942 - val_categorical_accuracy: 0.6738\n",
      "Epoch 12/15\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.00012245643455377955.\n",
      "408/408 [==============================] - 16s 40ms/step - loss: 0.6201 - categorical_accuracy: 0.7593 - val_loss: 0.7920 - val_categorical_accuracy: 0.6898\n",
      "Epoch 13/15\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 3.688316883663751e-05.\n",
      "408/408 [==============================] - 16s 39ms/step - loss: 0.6052 - categorical_accuracy: 0.7617 - val_loss: 0.7870 - val_categorical_accuracy: 0.6791\n",
      "Epoch 14/15\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 1.0051835886692629e-05.\n",
      "408/408 [==============================] - 17s 40ms/step - loss: 0.6055 - categorical_accuracy: 0.7673 - val_loss: 0.7874 - val_categorical_accuracy: 0.6765\n",
      "Epoch 15/15\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 2.4787521134827945e-06.\n",
      "408/408 [==============================] - 16s 39ms/step - loss: 0.5919 - categorical_accuracy: 0.7654 - val_loss: 0.7874 - val_categorical_accuracy: 0.6791\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "history = model.fit(train_dataset, validation_data=val_dataset, \n",
    "                    epochs=15, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178/178 [==============================] - 6s 33ms/step - loss: 0.7024 - categorical_accuracy: 0.7277\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7024242877960205, 0.7276566028594971]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating our model on the test set\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 – Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "# Performing inference with our model\n",
    "predictions = model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFSequenceClassifierOutput(loss=None, logits=array([[-1.370903  , -4.7255187 , -0.5999131 ,  3.622602  ],\n",
      "       [ 1.3311445 , -1.4883113 , -0.37181637, -0.26223233],\n",
      "       [-1.1938188 , -1.2004251 , -4.824672  ,  4.268196  ],\n",
      "       ...,\n",
      "       [ 0.6109057 , -1.8429809 , -2.5267117 ,  1.897653  ],\n",
      "       [ 3.3085601 , -2.588659  , -2.983421  , -0.5218434 ],\n",
      "       [-3.7007992 ,  3.6551635 , -0.12156612, -1.4982461 ]],\n",
      "      dtype=float32), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "# Inspecting our predictions\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Converting predicted logits to probabilities\n",
    "predictions = tf.nn.softmax(predictions.logits)\n",
    "\n",
    "# Extracting the indices with the highest probabilities\n",
    "predictions = tf.argmax(predictions, axis=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Converting numerical labels to their corresponding class names\n",
    "predictions = [class_names[prediction] for prediction in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sadness', 'anger', 'sadness', 'joy', 'sadness', 'anger', 'sadness', 'sadness', 'sadness', 'anger']\n"
     ]
    }
   ],
   "source": [
    "# Inspecting predicted class names\n",
    "print(predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: [CLS] @ user interesting choice of words... are you confirming that governments fund # terrorism? bit of an open door, but still... [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Predicted class: anger\n",
      "\n",
      "Tweet: [CLS] my visit to hospital for care triggered # trauma from accident 20 + yrs ago and image of my dead brother in it. feeling symptoms of # depression [SEP] [PAD] [PAD] [PAD] [PAD]\n",
      "Predicted class: sadness\n",
      "\n",
      "Tweet: [CLS] @ user welcome to # mpsvt! we are delighted to have you! # grateful # mpsvt # relationships [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Predicted class: joy\n",
      "\n",
      "Tweet: [CLS] what makes you feel # joyful? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Predicted class: sadness\n",
      "\n",
      "Tweet: [CLS] # deppression is real. partners w / # depressed people truly dont understand the depth in which they affect us. add in # anxiety & amp ; makes [SEP]\n",
      "Predicted class: sadness\n",
      "\n",
      "Tweet: [CLS] i am revolting. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Predicted class: anger\n",
      "\n",
      "Tweet: [CLS] rin might ever appeared gloomy but to be a melodramatic person was not her thing. \\ n \\ nbut honestly, she missed her old friend [SEP]\n",
      "Predicted class: sadness\n",
      "\n",
      "Tweet: [CLS] in need of a change! # restless [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Predicted class: sadness\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A function containing the transformation steps from above\n",
    "def logits_to_class_names(predictions):\n",
    "    predictions = tf.nn.softmax(predictions.logits)\n",
    "    predictions = tf.argmax(predictions, axis=1).numpy()\n",
    "    predictions = [class_names[prediction] for prediction in predictions]\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Retrieving a single test batch\n",
    "test_batch = next(iter(test_dataset))[0]\n",
    "\n",
    "# Obtaining predicted class names\n",
    "sample_predictions = logits_to_class_names(model(test_batch))\n",
    "\n",
    "# Printing sequences and corresponding labels\n",
    "for i in range(len(test_batch[\"input_ids\"])):      \n",
    "    print(f\"Tweet: {tokenizer.decode(test_batch['input_ids'][i])}\")\n",
    "    print(f\"Predicted class: {sample_predictions[i]}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
